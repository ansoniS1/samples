// References:
// https://support.dataset.com/hc/en-us/articles/360043295814-Optimized-Settings-for-the-Scalyr-Agent
// https://github.com/scalyr/scalyr-agent-2/blob/master/scalyr_agent/configuration.py
// For max file descriptors https://www.cyberciti.biz/faq/linux-increase-the-maximum-number-of-open-files/

{
  // Compression
  // Use deflate rather than bz2, because bz2 is too CPU intensive and for high log volumes
  // this will cause the agent to fall behind and eventually start dropping logs
  compression_type: "deflate",

  // if cpu is too high, the compression level can be reduced to 3
  compression_level: 6,

  // High log volume
  // the maximum request size to send (server max is 6000000)
  max_allowed_request_size: 5900000, 

  // Turn on 'pipelining' when request size reaches 'pipeline_threshold' of max_allowed_request_size
  // Pipelining means that the agent will begin preparing the next request while the current request
  // is in flight.
  // When set to 0.1, if the requests being sent to the server reach 10% of the max allowed request
  // size then pipelining will be enabled.  Set to 0 to always pipeline; this may increase CPU usage.
  pipeline_threshold: 0.0, 

  // The minimum amount of time to wait between sending requests
  // by default this is 1 second. Setting it lower will send more often, but may lead to increased CPU usage
  // If latency is still too high you can try reducing this below 0.5
  min_request_spacing_interval: 0.0, 

  // The maximum amount of time to wait between sending requests
  // by default this is 5 seconds, set lower to reduce waiting between sending requests
  max_request_spacing_interval: 5.0, 

  // `max_log_offset_size` and `max_existing_log_offset_size`
  // control how far behind the end of a log file the scalyr agent is allowed to
  // get before it drops bytes and skips to the end.  They should usually be set
  // in unison to a value that is a function of expected log volume per second,
  // with enough room to handle bursts of data.
  //
  // For example if they are set to 50000000 (50 MB) and you have 100 MB burst
  // of log data, then when the burst happens the agent will see it is too far
  // behind the end of the log and drop those logs, even though it might have
  // been able to gradually upload the burst data over 20-30 seconds.  Here we
  // set them to 200 MB which should be a reasonable setting for handling a high
  // volume of total log data (i.e. 5 MB/s per agent) with room for occasional
  // bursts > 100 MB/s
  max_log_offset_size: 200000000,   
  max_existing_log_offset_size: 200000000,
  max_line_size: 49900,

  // Multithreading
  use_multiprocess_workers: true,
  default_sessions_per_worker: 3,
  disable_max_send_rate_enforcement_overrides: true,
  max_new_log_detection_time: 30,
  config_change_check_interval: 60
}
